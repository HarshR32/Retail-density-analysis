{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f626525e-b0c5-4c2e-8ccf-c6f4b368bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.types import DoubleType\n",
    "from urllib3.util.retry import Retry\n",
    "from pyspark.sql.functions import col, substring_index\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from requests.adapters import HTTPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412f9865-b176-451e-a1e2-44bdc6635757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataframe(df, size: int):\n",
    "    return df.limit(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076172bc-ee14-4153-a569-d131eb9c0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_load_data(INPUT_JSON: str, OVERPASS_URL: str, overpass_query: str):\n",
    "    try:\n",
    "        with open(INPUT_JSON) as f:\n",
    "            data = json.load(f)\n",
    "        print(\"Loaded cached data\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Fetching fresh data...\")\n",
    "        response = requests.get(OVERPASS_URL, params={\"data\": overpass_query})\n",
    "        data = response.json()\n",
    "        with open(INPUT_JSON, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(\"Saved new data\")\n",
    "    elements = data[\"elements\"]\n",
    "    shops_data = []\n",
    "    for element in elements:\n",
    "        shop = {\n",
    "            \"id\": element.get(\"id\", None),\n",
    "            \"lat\": element.get(\"lat\", None),\n",
    "            \"lon\": element.get(\"lon\", None),\n",
    "            \"name\": element.get(\"tags\", {}).get(\"name\", None),\n",
    "            \"shop\": element.get(\"tags\", {}).get(\"shop\", None),\n",
    "        }\n",
    "        shops_data.append(shop)\n",
    "    return spark.createDataFrame(shops_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "294af105-03f7-4bc5-a4e3-d17ab9b58a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.dropna(subset = [\"name\", \"shop\"])\n",
    "    df = df.filter((col(\"lat\").isNotNull()) & (col(\"lon\").isNotNull()) &(col(\"lat\").between(-90, 90)) & (col(\"lon\").between(-180, 180))).withColumn(\"lat\", col(\"lat\").cast(\"double\")).withColumn(\"lon\", col(\"lon\").cast(\"double\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8b57bb1-9c7d-4a4f-84a9-40be0666c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_locations(df):\n",
    "    df = df.dropna(subset = [\"city\", \"state\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c501ef-a57b-41ce-ad28-773fcc10b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocode(lat, lon, cache, last_req):\n",
    "    key = f\"{lat:.5f}, {lon:.5f}\"\n",
    "    if key in cache: return cache[key]\n",
    "    if time.time() - last_req[0] < 1.1:\n",
    "        time.sleep(1.1 - (time.time() - last_req[0]))\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent = \"retail-analysis/2.0\", timeout = 15)\n",
    "        location = geolocator.reverse((lat, lon), zoom = 13, exactly_one = True)\n",
    "        address = location.raw.get(\"address\", {}) if location else {}\n",
    "        city = address.get(\"city\") or address.get(\"town\") or address.get(\"village\") or None\n",
    "        state = address.get(\"state\", None)\n",
    "        result = (city[:15],state[:15]) if city and state else (city, state)\n",
    "    except Exception as e:\n",
    "        result = (\"error\",str(e)[:15])\n",
    "    cache[key] = result\n",
    "    last_req[0] = time.time()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa9172e4-527c-44a4-8bc5-be66ab3f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(df):\n",
    "    cache = {}\n",
    "    last_req = [time.time()]\n",
    "    rows = df.collect()\n",
    "\n",
    "    with tqdm(total = len(rows), desc = \"Geocoding\", bar_format = \"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
    "        results = []\n",
    "        for row in rows:\n",
    "            city, state = reverse_geocode(row.lat, row.lon, cache, last_req)\n",
    "            results.append(Row(id = row.id, lat = row.lat, lon = row.lon, name = row.name, shop = row.shop, city = city, state = state))\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Cache: {len(cache)} Last: {city}\")\n",
    "        return spark.createDataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d10035-8837-4c05-b6d0-6a6c543fb587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Loaded cached data\n",
      "Started cleaning processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 20:27:58 WARN TaskSetManager: Stage 0 contains a task of very large size (6854 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/05 20:28:03 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------------------+-----------+\n",
      "|       id|       lat|       lon|                name|       shop|\n",
      "+---------+----------+----------+--------------------+-----------+\n",
      "|171171134|30.8806773|75.8474324|Jassal Department...|convenience|\n",
      "|171277546|30.8982473|75.8194336|    Vishal Mega Mart|supermarket|\n",
      "|171313618|30.8974153| 75.820184|  Ansal Plaza;Basant|supermarket|\n",
      "|181316289|30.8687898|75.8838422| Ashok Karyana Store|convenience|\n",
      "|181316582|30.8590486|75.8827218|       Grewal Market|       hifi|\n",
      "|243970408|30.8358801|75.9703788|               Jhajj|      dairy|\n",
      "|245606842| 12.444432|80.1089115|CINDURA'S Beauty ...|     beauty|\n",
      "|245748968|26.4998975|83.7829787|              Deoria|video_games|\n",
      "|246912870|30.8911208|75.8391201|     Satpal Di Hatti|convenience|\n",
      "|246914014|30.8924466|75.8434116|     Satpal Di Hatti|convenience|\n",
      "|252417084| 8.5265181|77.0364515|          Nava Yatra|     bakery|\n",
      "|263575322|30.8656351|75.8723119|Robert Clinical L...|convenience|\n",
      "|263575929|30.8907167|75.8283212|Dr. Kumar's Homeo...|convenience|\n",
      "|266435424|28.4624078|77.0874735|     DLF Supermart 1|supermarket|\n",
      "|266472673|12.9174059|  77.64853|Priti Departmenta...|convenience|\n",
      "|266472732|12.9176987|77.6515426|Adeshwara Electro...|electronics|\n",
      "|266472834|12.9192611|77.6518197| I-gate optic vision|     optics|\n",
      "|266687182|28.4671709| 77.081805|        DLF Galleria|supermarket|\n",
      "|266729534| 28.417077|77.0466108|      Block B Market|supermarket|\n",
      "|266729875|28.4173675|77.0510414|      Block D Market|supermarket|\n",
      "+---------+----------+----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"RetailDensityAnalysis\").config(\"spark.sql.shuffle.partitions\", \"1\").config(\"spark.default.parallelism\", \"1\").getOrCreate()\n",
    "    \n",
    "    INPUT_JSON = \"../dataset/overpass_data.json\"\n",
    "    OUTPUT_CSV = \"../dataset/retail_density_results\"\n",
    "    AGG_OUTPUT_CSV = \"../dataset/retail_density_agg_results\"\n",
    "    # OUTPUT_CSV_UTM = \"./retail_density_results\"\n",
    "    # AGG_OUTPUT_CSV_UTM = \"./retail_density_agg_results\"\n",
    "    OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "    DATASET_SIZE = 30000\n",
    "    \n",
    "    overpass_query = \"\"\"\n",
    "    [out:json];\n",
    "    area[\"name\"=\"India\"]->.searchArea;\n",
    "    (\n",
    "        node[\"shop\"](area.searchArea);\n",
    "        way[\"shop\"](area.searchArea);\n",
    "        relation[\"shop\"](area.searchArea);\n",
    "    );\n",
    "    out center;\"\"\"\n",
    "    \n",
    "    # load and extract data\n",
    "    print(\"loading data...\")\n",
    "    df = extract_and_load_data(INPUT_JSON, OVERPASS_URL, overpass_query)\n",
    "\n",
    "    # clean data\n",
    "    print(\"Started cleaning processes...\")\n",
    "    clean_df = clean_data(df)\n",
    "    clean_df = slice_dataframe(clean_df, DATASET_SIZE)\n",
    "\n",
    "    # add location data\n",
    "    print(\"Extracting Geolocation...\")\n",
    "    print(f\"Processing {clean_df.count()} locations...\")\n",
    "    start = time.time()\n",
    "    df_with_location = extract_location(clean_df)\n",
    "    print(f\"/nCompleted in {time.time() - start:.1f}s\")\n",
    "\n",
    "    print(\"Cleaning DataFrame of null locations...\")\n",
    "    clean_df_with_location = clean_data_locations(df_with_location).sort('city', 'name')\n",
    "\n",
    "    print(\"aggregating data for analysis...\")\n",
    "    agg_df = clean_df_with_location.groupBy('city').count().sort('city')\n",
    "\n",
    "    # write out the csv files\n",
    "    print(\"Writing csv files...\")\n",
    "    \n",
    "    print(f\"Writing {OUTPUT_CSV}\")\n",
    "    clean_df_with_location.write.mode(\"overwrite\").option(\"header\",True).csv(OUTPUT_CSV) \n",
    "    \n",
    "    # print(f\"Writing {OUTPUT_CSV_UTM}\")\n",
    "    # clean_df_with_location.write.mode(\"overwrite\").option(\"header\",True).csv(OUTPUT_CSV_UTM)   \n",
    "    \n",
    "    print(f\"Writing {AGG_OUTPUT_CSV}\")\n",
    "    agg_df.write.mode(\"overwrite\").option(\"header\",True).csv(AGG_OUTPUT_CSV)\n",
    "    \n",
    "    # print(f\"Writing {AGG_OUTPUT_CSV_UTM}\")\n",
    "    # agg_df.write.mode(\"overwrite\").option(\"header\",True).csv(AGG_OUTPUT_CSV_UTM)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49c8d3-9545-4e3c-9279-8e14b15855ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
